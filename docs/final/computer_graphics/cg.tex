\chapter{Computer Graphics}
The graphics of the program are handled via LWJGL, an OpenGL binder for Java that also supports basic input and sound. The program targets OpenGL 3.0 core version, this means that: all shaders and matrix computations are done by the Java program, most advanced OpenGL features are supported and hardware support is readily available through the TU/E notebooks. 
The graphics architecture is based around a forward rendering approach as opposed to a deferred rendering approach. Deferred rendering was considered for its ability to handle multiple light sources efficiently but was eventually rejected for its limited expected performance on our hardware, as well as a lack of experience with implementing this architecture. Instead a forward rendering architecture was modified to support multiple light sources efficiently. This section will describe the graphics features in more detail.

\section{Lighting}
The lighting in the program supports multiple dynamic lights as well as shadows and bump mapping.  Before drawing anything, a setup phase is used to provide world-space information about lighting to simplify further calculations. Without this pass all lights have to be considered for all rendered pixels, which requires a lot of computation. Upon drawing the lighting model is applied to provide realistic per-fragment shading and the overall detail is improved by using bump mapping.

\subsection{Setup}
For the setup phase, all visible lights are gathered and their influence is calculated for every cell in a grid spanning the view. The data of all these lights (radius, color, specular color, direction, cutoffangle, worldposition) as well as the data contained in the grid is then uploaded to the graphics card. This happens every frame to allow for dynamic lighting.  

\subsection{Ligting Model}
To provide aesthetically pleasing and semi-realistic lighting, a per-fragment Phong-Blinn lighting scheme is used with a fixed amount of maximum lights which influence a single fragment. For every fragment the lambertian model for diffuse lighting is applied, followed by a blinn calculation for the specular component. This lighting model is extended with several fall-off functions which modulate the light based on distance, light radius, light direction and cutoff angle. The last step in the lighting model is a shadow map test, which decides whether all gathered lighting information for the particular light is written to the framebuffer or discarded. This model supports many lights per scene (currently 128), as well as shadow maps, spotlights, point lights and colored lights. Wrapping this much functionality reduces the number of shader switches the OpenGL context needs to make, which increases performance. It also inscreases client-side code readability, because all geometry and all lights are processed 
equally.

\subsection{Bump Mapping}
Increasing graphical fidelity by using high quality models is both computationally and time expensive. However, this does not change the fact that high graphical fidelity is still desirable. One of the ways to increase the quality of the scene without heavily burdening the hardware or the model designers is using bump mapping. Bump mapping tries to increase the quality of a model without improving the quality of the shape of the model. Instead it tries to improve the quality of the pixels computed from the model by manipulating its normals.
The lighting model uses a normal both for the computation of the Lambertian component and the Blinn specular component. Modulating this normal per fragment, via a texture, can greatly enhance the visual fidelity of any scene. The calculation of this normal can be achieved by using a tangent-to-world-space matrix, which can be constructed inside the fragment or vertex shader provided we have access to the normal and tangent of the face being processed. A specially made texture, a normal map, can then be used as input for this matrix transformation to perform proper per-fragment normal modulation. The beauty of this method is that the lighting model does not have to change. Instead of giving the lighting model the same normal for every fragment in a triangle, we give it a different normal. All calculations are still valid.

\section{Shadow Mapping}
The graphics architecture supports a number of dynamic lights, the decision was made early on to complement these lights with fitting shadows to improve the realism. When considering shadows, the two main techniques used in computer graphics are shadow mapping and shadow volumes. Shadow volumes produce pixel perfect shadows while requiring pre-computation on the CPU (or possibly on the GPU using a geometry  shader, an unsupported feature for our target platform). Shadow mapping produces lower quality shadows but they can be computed entirely on the GPU. Since the CPU would be busy with AI processes, shadow mapping was picked.

Shadow mapping consists of two phases. The first phase is a depth pre-pass where the entire scene is rendered from the point of view of the light into a buffer (the shadow map). The second phase is when this shadow map is used while drawing the scene the ``normal'' way to compute whether a single fragment is in shadows or not.

The first phase starts with setting up a ModelViewProjection matrix for the perspective of the light. This matrix is used to transform every vertex in the scene to the screenspace of the light, effectively taking a picture from the lights perspective. The picture itself does not contain any colors of the scene, instead it contains the depth of the scene for every pixel the light can ``see''.

In the second phase, the normal render phase, the same ModelViewProjection matrix is used once again, transforming every vertex in the scene to the screenspace of the light. The depth of this pixel is then compared to the depth in the shadow map. If there is any occluder between the light and this pixel, the depth found in the shadowmap should be different to the calculated depth. This allows the shader to distinguish between lit and shadowed pixels.

This setup can easily be extended to support multiple lights by partitioning the shadow map into equal parts for each light, and translating the vertices with each of the lights ModelViewProjection matrices. Fillrate or resolution will suffer heavily when too many lights are casting shadows, limiting the amount of shadowcasters, therefore the lights which do cast shadows should be carefully selected.

\section{Geometry}
Almost all geometry used within the program is loaded from external model files and converted into a usable format for OpenGL.

\subsection{Models}
Models are imported into the engine through the .obj specification. This file specification describes a generic way to form any mesh from a set of vertices, normals, texture coordinates and faces, and has the additional advantage of being a plaintext specification (as opposed to bytecode), which makes it easy to read and debug. When loaded into memory, the data contained within the original .obj file is expanded upon by calculating a corresponding tangent vector with each normal. This resulting dataset is then uploaded to the graphics card in a tightly packed buffer, and is now ready for use via a simple glDrawArrays call.

\subsection{Animations}
After considering skeletal animations, a technique where each vertex of the model is displaced depending on the displacement of a corresponding virtual ``bone'', and after consecutively dismissing this method as overly complicated for our limited quality models, the decision was made to implement the most straightforward animation technique: keyframe animations. In keyframe animations, a new set of vertices is created for every instance of a discrete timestep of the animation. The entire animation is simply a collection of models, and can be treated as such. By using this method, the code for displaying animations is nearly identical to the code needed to display a static model, the only difference being that the drawn model instance changes every (couple of) frames.

\subsection{Particles}
To accommodate the post-apocalyptic setting we wanted to use fire as both a light source and interesting element of the scenery. Particles were used to produce a fairly realistic geometrical representation of fire. To enable this we defined emitters which produce partially randomized particles within the specified parameters. Each particle receives an initial speed vector, a force vector affecting it's trajectory and a transformation to be applied each frame. The transformation can be used to have the individual particles rotate while moving. 

\subsubsection{Shape}
Any type of previously defined geometry could be used as representation of a single particle. However we chose to use the simplest of renderable geometries, a single triangle, for our fire particles. This way we could support more particles and create a more dynamic looking flame. By continuously rotating the triangles they would still look good from all angles and look more volatile and unpredictable as is the case with real fire.

\subsubsection{Look}
When the resulting set of particles is rendered using a solid colour it does not give the desired impression however. Using a single colour makes particles difficult to distinguish which is not necessarily a bad thing but the hard edges do not look very fire-like. One solution would have been to use semi-translucent textures to soften the edges. But allowing for these types of textures would have been costly. Instead we decided to use a post-processing to blur the particles final image, this effect is further described under Bloom. Another improvement over the solid coloured particles was to introduce a gradient from which a colour is picked based on how long a particle has existed. This is consistent with how the colour of actual fire changes based on temperature and in turn distance from the source.

\subsubsection{Performance}
All in all there is quite a bit of data to be generated with each emitted particle. Processing power can be a bit of a bottleneck. By using lookup tables for certain particle attributes we could maximize the efficiency. This was done in generating a random direction and rotation. The gradient class we implemented also defines an limited array of colours. Instead of recalculating the exact colour for each value $t ; 0 <= t < 1$ we take the closest from the predefined array.

\section{Post Processing}
After rendering the scene and all of its geometry, extra graphic effects can be applied to the final result. This is called post processing.
\subsection{FXAA}
To combat aliasing effects, where pixel contrast at the edges of a polygon produces ugly jagged lines, an argument could be made to implement FXAA (or another form of AA) as a post process pass to nearly any computer graphics application. Fast approXimate Anti Aliasing is a technique which can hide these type of jagged edges by applying a blur in the direction perpendicular to the jagged line. This technique makes it much faster than competing AA techniques like super sampling. An example implementation for FXAA is available online from the creator of FXAA, this implementation can relatively easily be retrofitted to work with our engine. The decision not to use this technique was made because as development advanced it appeared that the mentioned jagged lines were not as eye catching as anticipated.
\subsection{Bloom}
When implementing the particle system it was difficult trying to blend the particles with the background. Without blending the particles would seem unnatural and solid, while blending the particles proved difficult as they need to be sorted to be properly blended. With thousands of particles on the screen, sorting them was not viable. This is where bloom comes into play. Instead of drawing the particles to the screen with all other geometry, they are drawn to a separate texture. This texture is then blurred before it is drawn to the screen. The particles now look soft and natural, even more so than with regular blending, without requiring the expensive blending or sorting.   
